{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "93c5a833",
   "metadata": {},
   "source": [
    "## This will be our initial attempt to develop a script to scrape the website of choice."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "ada06364",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import requests \n",
    "from bs4 import BeautifulSoup \n",
    "    \n",
    "def getdata(url): \n",
    "    r = requests.get(url) \n",
    "    return r.text \n",
    "        \n",
    "# htmldata = getdata(\"https://en.wikipedia.org/wiki/Cheese\")\n",
    "htmldata = getdata(\"https://www.csulb.edu/college-of-education\")\n",
    "soup = BeautifulSoup(htmldata, 'html.parser')\n",
    "\n",
    "tgt_img_urls = []\n",
    "    \n",
    "for item in soup.find_all('img'):\n",
    "    if item['src'][:8] == 'https://':\n",
    "        tgt_img_urls.append(item['src'])\n",
    "    elif item['src'][:2] == '//':\n",
    "        https_concat = 'https:'+(item['src'])\n",
    "        tgt_img_urls.append(https_concat)\n",
    "        print(tgt_img_urls)\n",
    "    elif item['src'][:1] == '/':\n",
    "        https_concat = 'https:/'+(item['src'])\n",
    "        tgt_img_urls.append(https_concat)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "fc145d77",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "error\n",
      "error\n",
      "error\n",
      "error\n",
      "error\n",
      "error\n",
      "error\n",
      "error\n",
      "error\n",
      "error\n",
      "error\n",
      "error\n",
      "error\n",
      "error\n",
      "error\n",
      "error\n",
      "error\n",
      "error\n",
      "error\n",
      "error\n",
      "error\n",
      "error\n",
      "error\n"
     ]
    }
   ],
   "source": [
    "# credit to Nate who found this and made it work for us!\n",
    "import os\n",
    "import requests\n",
    "import shutil\n",
    "\n",
    "# create a directory to store what we will download\n",
    "# user will need to define their own desired destination folder\n",
    "os.mkdir('C:\\\\Users\\\\hanse\\\\Documents\\\\GitHub\\\\Algorithmic-Bias\\\\Web-Scraping\\\\image-folder2')\n",
    "os.chdir('C:\\\\Users\\\\hanse\\\\Documents\\\\Github\\\\Algorithmic-Bias\\\\Web-Scraping\\\\image-folder2')\n",
    "\n",
    "# more robust way of doing stuff above\n",
    "# def getdata(url):\n",
    "#     folder_name = input('File Name:- ')\n",
    "#     if not os.path.exists(folder_name):\n",
    "#         os.makedirs(folder_name) \n",
    "#     os.chdir(f'{folder_name}')\n",
    "#     r = requests.get(url) \n",
    "#     return r.text\n",
    "\n",
    "urls = ['https://upload.wikimedia.org/wikipedia/commons/thumb/a/a8/Cheese_platter.jpg/520px-Cheese_platter.jpg'\n",
    "       ,'https://upload.wikimedia.org/wikipedia/commons/thumb/5/5f/Ricotta_affumicata_della_sila.jpg/440px-Ricotta_affumicata_della_sila.jpg'\n",
    "       ,'https://upload.wikimedia.org/wikipedia/commons/thumb/4/48/Formaggi.JPG/440px-Formaggi.JPG']\n",
    "for x in tgt_img_urls:\n",
    "    try:\n",
    "        filename = x.split(\"/\")[-1]\n",
    "\n",
    "    # Open the url image, set stream to True, this will return the stream content.\n",
    "        r = requests.get(x, stream = True)\n",
    "\n",
    "    # Check if the image was retrieved successfully\n",
    "        if r.status_code == 200:\n",
    "        # Set decode_content value to True, otherwise the downloaded image file's size will be zero.\n",
    "            r.raw.decode_content = True\n",
    "\n",
    "        # Open a local file with wb ( write binary ) permission.\n",
    "            with open(filename,'wb') as f:\n",
    "                shutil.copyfileobj(r.raw, f)\n",
    "\n",
    "            print('Image sucessfully Downloaded: ',filename)\n",
    "        else:\n",
    "            print('Image Couldn\\'t be retreived')\n",
    "    except:\n",
    "        print('error')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4004dd3e",
   "metadata": {},
   "source": [
    "# <font color='red'> stuff above didnt work fully, stuff below we are adapting "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d168081f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "3fd70645",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Extracting images: 100%|███████████████████████████████████████████████████████████████████████| 23/23 [00:00<?, ?it/s]\n",
      "Downloading C:\\Users\\hanse\\Documents\\Github\\Algorithmic-Bias\\Web-Scraping\\image-folder\\lb.svg: 100%|█| 3.94k/3.94k [00:\n",
      "Downloading C:\\Users\\hanse\\Documents\\Github\\Algorithmic-Bias\\Web-Scraping\\image-folder\\logo-footer.svg: 100%|█| 7.84k/7\n",
      "Downloading C:\\Users\\hanse\\Documents\\Github\\Algorithmic-Bias\\Web-Scraping\\image-folder\\mobile-footer-logo.png: 100%|█| \n",
      "Downloading C:\\Users\\hanse\\Documents\\Github\\Algorithmic-Bias\\Web-Scraping\\image-folder\\wordmark-black.png: 100%|█| 14.4\n",
      "Downloading C:\\Users\\hanse\\Documents\\Github\\Algorithmic-Bias\\Web-Scraping\\image-folder\\wordmark-black.png: 100%|█| 14.4\n",
      "Downloading C:\\Users\\hanse\\Documents\\Github\\Algorithmic-Bias\\Web-Scraping\\image-folder\\banner_ced_thy-148-teacher-middl\n",
      "Downloading C:\\Users\\hanse\\Documents\\Github\\Algorithmic-Bias\\Web-Scraping\\image-folder\\banner_ced_edu-week.jpg: 100%|█|\n",
      "Downloading C:\\Users\\hanse\\Documents\\Github\\Algorithmic-Bias\\Web-Scraping\\image-folder\\nina-flores-class5-b_1.jpg: 100%\n",
      "Downloading C:\\Users\\hanse\\Documents\\Github\\Algorithmic-Bias\\Web-Scraping\\image-folder\\banner_ced_apply2-braclets-and-p\n",
      "Downloading C:\\Users\\hanse\\Documents\\Github\\Algorithmic-Bias\\Web-Scraping\\image-folder\\CED%20Main.jpg: 100%|█| 306k/306\n",
      "Downloading C:\\Users\\hanse\\Documents\\Github\\Algorithmic-Bias\\Web-Scraping\\image-folder\\Banner_CED_EDADHooding.jpg: 100%\n",
      "Downloading C:\\Users\\hanse\\Documents\\Github\\Algorithmic-Bias\\Web-Scraping\\image-folder\\Banner_CED_EDDHooding23.jpg: 100\n",
      "Downloading C:\\Users\\hanse\\Documents\\Github\\Algorithmic-Bias\\Web-Scraping\\image-folder\\Main%20commence.jpg: 100%|█| 88.\n",
      "Downloading C:\\Users\\hanse\\Documents\\Github\\Algorithmic-Bias\\Web-Scraping\\image-folder\\Banner_CED_ChloeHaynes.jpg: 100%\n",
      "Downloading C:\\Users\\hanse\\Documents\\Github\\Algorithmic-Bias\\Web-Scraping\\image-folder\\Banner_CED_AnnaOrtiz.jpg: 100%|█\n",
      "Downloading C:\\Users\\hanse\\Documents\\Github\\Algorithmic-Bias\\Web-Scraping\\image-folder\\VegaTest.jpg: 100%|█| 123k/123k \n",
      "Downloading C:\\Users\\hanse\\Documents\\Github\\Algorithmic-Bias\\Web-Scraping\\image-folder\\ced_fact_sheet_final_page_2_0.jp\n",
      "Downloading C:\\Users\\hanse\\Documents\\Github\\Algorithmic-Bias\\Web-Scraping\\image-folder\\accolade.png: 100%|█| 12.5k/12.5\n",
      "Downloading C:\\Users\\hanse\\Documents\\Github\\Algorithmic-Bias\\Web-Scraping\\image-folder\\tr: 0.00B [00:00, ?B/s]\n",
      "Downloading C:\\Users\\hanse\\Documents\\Github\\Algorithmic-Bias\\Web-Scraping\\image-folder\\lb_wordmark_blk-59.svg: 100%|█| \n",
      "Downloading C:\\Users\\hanse\\Documents\\Github\\Algorithmic-Bias\\Web-Scraping\\image-folder\\csulb.svg: 100%|█| 8.75k/8.75k [\n",
      "Downloading C:\\Users\\hanse\\Documents\\Github\\Algorithmic-Bias\\Web-Scraping\\image-folder\\lb.svg: 100%|█| 3.94k/3.94k [00:\n",
      "Downloading C:\\Users\\hanse\\Documents\\Github\\Algorithmic-Bias\\Web-Scraping\\image-folder\\wordmark-white.png: 100%|█| 45.0\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "import os\n",
    "from tqdm import tqdm\n",
    "from bs4 import BeautifulSoup as bs\n",
    "from urllib.parse import urljoin, urlparse\n",
    "\n",
    "def is_valid(url):\n",
    "    \"\"\"\n",
    "    Checks whether `url` is a valid URL.\n",
    "    \"\"\"\n",
    "    parsed = urlparse(url)\n",
    "    return bool(parsed.netloc) and bool(parsed.scheme)\n",
    "\n",
    "def get_all_images(url):\n",
    "    \"\"\"\n",
    "    Returns all image URLs on a single `url`\n",
    "    \"\"\"\n",
    "    soup = bs(requests.get(url).content, \"html.parser\")\n",
    "    \n",
    "    urls = []\n",
    "    for img in tqdm(soup.find_all(\"img\"), \"Extracting images\"):\n",
    "        img_url = img.attrs.get(\"src\")\n",
    "        if not img_url:\n",
    "            # if img does not contain src attribute, just skip\n",
    "            continue\n",
    "        \n",
    "        # make the URL absolute by joining domain with the URL that is just extracted\n",
    "        img_url = urljoin(url, img_url)\n",
    "        \n",
    "        try:\n",
    "            pos = img_url.index(\"?\")\n",
    "            img_url = img_url[:pos]\n",
    "        except ValueError:\n",
    "            pass\n",
    "        \n",
    "        # finally, if the url is valid\n",
    "        if is_valid(img_url):\n",
    "            urls.append(img_url)\n",
    "    return urls\n",
    "\n",
    "def download(url, pathname):\n",
    "    \"\"\"\n",
    "    Downloads a file given an URL and puts it in the folder `pathname`\n",
    "    \"\"\"\n",
    "    # if path doesn't exist, make that path dir\n",
    "    if not os.path.isdir(pathname):\n",
    "        os.makedirs(pathname)\n",
    "    # download the body of response by chunk, not immediately\n",
    "    response = requests.get(url, stream=True)\n",
    "    # get the total file size\n",
    "    file_size = int(response.headers.get(\"Content-Length\", 0))\n",
    "    # get the file name\n",
    "    filename = os.path.join(pathname, url.split(\"/\")[-1])\n",
    "    # progress bar, changing the unit to bytes instead of iteration (default by tqdm)\n",
    "    progress = tqdm(response.iter_content(1024), f\"Downloading {filename}\", total=file_size, unit=\"B\", unit_scale=True, unit_divisor=1024)\n",
    "    with open(filename, \"wb\") as f:\n",
    "        for data in progress.iterable:\n",
    "            # write data read to the file\n",
    "            f.write(data)\n",
    "            # update the progress bar manually\n",
    "            progress.update(len(data))\n",
    "\n",
    "def main(url, path):\n",
    "    # get all images\n",
    "    imgs = get_all_images(url)\n",
    "    for img in imgs:\n",
    "        # for each image, download it\n",
    "        download(img, path)\n",
    "        \n",
    "main(\"https://www.csulb.edu/college-of-education\", \"C:\\\\Users\\\\hanse\\\\Documents\\\\Github\\\\Algorithmic-Bias\\\\Web-Scraping\\\\image-folder\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
