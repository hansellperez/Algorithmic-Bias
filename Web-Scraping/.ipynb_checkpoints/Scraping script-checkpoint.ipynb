{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "93c5a833",
   "metadata": {},
   "source": [
    "## This will be our initial attempt to develop a script to scrape the website of choice."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3fd70645",
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "import os\n",
    "from tqdm import tqdm\n",
    "from bs4 import BeautifulSoup as bs\n",
    "from urllib.parse import urljoin, urlparse\n",
    "\n",
    "def is_valid(url):\n",
    "    \"\"\"\n",
    "    Checks whether `url` is a valid URL.\n",
    "    \"\"\"\n",
    "    parsed = urlparse(url)\n",
    "    return bool(parsed.netloc) and bool(parsed.scheme)\n",
    "\n",
    "def get_all_images(url):\n",
    "    \"\"\"\n",
    "    Returns all image URLs on a single `url`\n",
    "    \"\"\"\n",
    "    soup = bs(requests.get(url).content, \"html.parser\")\n",
    "    \n",
    "    urls = []\n",
    "    for img in tqdm(soup.find_all(\"img\"), \"Extracting images\"):\n",
    "        img_url = img.attrs.get(\"src\")\n",
    "        if not img_url:\n",
    "            # if img does not contain src attribute, just skip\n",
    "            continue\n",
    "        \n",
    "        # make the URL absolute by joining domain with the URL that is just extracted\n",
    "        img_url = urljoin(url, img_url)\n",
    "        \n",
    "        try:\n",
    "            pos = img_url.index(\"?\")\n",
    "            img_url = img_url[:pos]\n",
    "        except ValueError:\n",
    "            pass\n",
    "        \n",
    "        # finally, if the url is valid\n",
    "        if is_valid(img_url):\n",
    "            urls.append(img_url)\n",
    "    return urls\n",
    "\n",
    "def download(url, pathname):\n",
    "    \"\"\"\n",
    "    Downloads a file given an URL and puts it in the folder `pathname`\n",
    "    \"\"\"\n",
    "    # if path doesn't exist, make that path dir\n",
    "    if not os.path.isdir(pathname):\n",
    "        os.makedirs(pathname)\n",
    "    # download the body of response by chunk, not immediately\n",
    "    response = requests.get(url, stream=True)\n",
    "    # get the total file size\n",
    "    file_size = int(response.headers.get(\"Content-Length\", 0))\n",
    "    # get the file name\n",
    "    filename = os.path.join(pathname, url.split(\"/\")[-1])\n",
    "    # progress bar, changing the unit to bytes instead of iteration (default by tqdm)\n",
    "    progress = tqdm(response.iter_content(1024), f\"Downloading {filename}\", total=file_size, unit=\"B\", unit_scale=True, unit_divisor=1024)\n",
    "    with open(filename, \"wb\") as f:\n",
    "        for data in progress.iterable:\n",
    "            # write data read to the file\n",
    "            f.write(data)\n",
    "            # update the progress bar manually\n",
    "            progress.update(len(data))\n",
    "\n",
    "\"\"\"\n",
    " \"main\" is the function that is actually scraping and downloading the images off a specified website\n",
    " input: a url and a directory path\n",
    " output/result: images are downloaded into specific folder\n",
    " everything above is used in main\n",
    "\"\"\"           \n",
    "def main(url, path):\n",
    "    # get all images\n",
    "    imgs = get_all_images(url)\n",
    "    for img in imgs:\n",
    "        # for each image, download it\n",
    "        download(img, path)\n",
    "\n",
    "\"\"\"\n",
    "example of how to run main to download images      \n",
    "main(\"https://www.csulb.edu/college-of-education\", \"C:\\\\Users\\\\hanse\\\\Documents\\\\Github\\\\Algorithmic-Bias\\\\Web-Scraping\\\\image-folder\")\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "72e5ff78",
   "metadata": {},
   "outputs": [],
   "source": [
    "from bs4 import BeautifulSoup\n",
    "import requests\n",
    "\n",
    "def get_embedded_url(url):\n",
    "    \"\"\"\n",
    "    extracts all links embedded in 'url' that contain \"college-of-education\"\n",
    "    stores the links in the list 'observed_links'\n",
    "    \"\"\"\n",
    "    page = requests.get(url)    \n",
    "    data = page.text\n",
    "    soup = BeautifulSoup(data)\n",
    "\n",
    "    # this is creating the list of urls that are embedded in college of ed homepage that we want to look into\n",
    "    observed_links = []\n",
    "    for link in soup.find_all('a'):\n",
    "        if 'college-of-education' in link.get('href'):\n",
    "            if 'https://www.csulb.edu' in link.get('href'):\n",
    "                observed_links.append(link.get('href'))\n",
    "            elif link.get('href')[:1] == '/':\n",
    "                https_concat = 'https://www.csulb.edu'+(link.get('href'))\n",
    "                observed_links.append(https_concat)\n",
    "    observed_links = list(set(observed_links))\n",
    "    return observed_links"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3fa3442a",
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "get all links embedded in \"https://www.csulb.edu/college-of-education w/ the condition that \n",
    "it has \"college-of-education\" in the url\n",
    "\n",
    "store the links in a list, \"observed_links\"\n",
    "''' \n",
    "observed_links = get_embedded_url(\"https://www.csulb.edu/college-of-education\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "98feb5eb",
   "metadata": {},
   "source": [
    "Next block is only for inspection purposes,user can skip to the next block"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b95817ff",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# display the list of links generated above, what does it \"physically\" look like?\n",
    "display(observed_links)\n",
    "\n",
    "# display the number of links that were extracted using get_embedded_url\n",
    "display(len(observed_links))\n",
    "\n",
    "# count the number of \"valid\" or executable links in the observed\n",
    "count = 0\n",
    "for i in observed_links:\n",
    "    if is_valid(i):\n",
    "        count += 1\n",
    "count"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "16c07c0f",
   "metadata": {},
   "source": [
    "### Image Download\n",
    "The following code block is where I actually begin to download images.\n",
    "The loop goes through the list of interest developed, \"observed_links\", above and runs \"main\" to get the image on the url specified (Note: the url specified is inside the list \"observed_links\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d8f8018f",
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(len(observed_links)):\n",
    "    main(observed_links[i], \"C:\\\\Users\\\\hanse\\\\Documents\\\\Github\\\\Algorithmic-Bias\\\\Web-Scraping\\\\image-folder\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f3d43138",
   "metadata": {},
   "source": [
    "Next block is only for inspection purposes,user can skip to the next block"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "42d85c89",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "File count: 82\n"
     ]
    }
   ],
   "source": [
    "# count the files in the folder that was the target of my downloads\n",
    "dir_path = \"C:\\\\Users\\\\hanse\\\\Documents\\\\Github\\\\Algorithmic-Bias\\\\Web-Scraping\\\\image-folder\"\n",
    "count = 0\n",
    "# Iterate directory\n",
    "for path in os.listdir(dir_path):\n",
    "    # check if current path is a file\n",
    "    if os.path.isfile(os.path.join(dir_path, path)):\n",
    "        count += 1\n",
    "print('File count:', count)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e5fa2fe6",
   "metadata": {},
   "source": [
    "The process above yields 81 searchable College of Education webpages and 82 image downloads. 1 image is actually repeated 4x so the net total image downloads for the identified webpages is effectively 78.\n",
    "\n",
    "Next we will try to expand on the list of searchable webpages by running the function \"<span style=\"color:blue\">get_embedded_url</span>\" for each of the 81 searchable webpages identified above. (Note: The 81 webpages ID'd above are embedded in the College of Education Home page.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "87d6225a",
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "observed_links_lvl2 is a nested list that stores a \"level 2\" scrape of embedded links.\n",
    "i.e. for each webpage url in \"observed_links\" we extract all embedded links that satisfy our criteria.\n",
    "    - such criteria are provided in the \"get_embedded_url\" function definition block of code\n",
    "'''\n",
    "observed_links_lvl2 = [None]*len(observed_links)\n",
    "for i in range(len(observed_links)):\n",
    "    observed_links_lvl2[i] = get_embedded_url(observed_links[i])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e46434f4",
   "metadata": {},
   "source": [
    "Next block is only for inspection purposes,user can skip to the next block"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8dec3c62",
   "metadata": {},
   "outputs": [],
   "source": [
    "# count the number of links in the nested\n",
    "for i in range(len(observed_links_lvl2)):\n",
    "    print(len(observed_links_lvl2[i]))\n",
    "    \n",
    "display(observed_links_lvl2[:2])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7cfe5428",
   "metadata": {},
   "source": [
    "clean up the \"level 2\" url scrape data by flattening out the nested list, \"observed_links2\" and remove repetitions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "6f60a088",
   "metadata": {},
   "outputs": [],
   "source": [
    "# flatten observed_links2\n",
    "from itertools import chain\n",
    "lvl2_flat = list(chain.from_iterable(observed_links_lvl2))\n",
    "\n",
    "# keep only unique urls\n",
    "lvl2_flat_unique = list(set(lvl2_flat))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "778c5c61",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "749"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# inspect size of lists created above\n",
    "display(len(lvl2_flat))\n",
    "len(lvl2_flat_unique)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "832f9196",
   "metadata": {},
   "source": [
    "The following code block served to remove the \"observed_links\" list from \"lvl2_flat_unique\". This is done for the purpose of the scope of the CSULB DST tasks. Images from \"observed_links\" are already present in out repo so don't want to repeat downloads from these links because it take \"a good while\" so we remove them from the \"to-be downloaded\" list."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "febec0e2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "668"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from collections import Counter\n",
    "to_be_downloaded = list((Counter(lvl2_flat_unique)-Counter(observed_links)).elements())\n",
    "display(len(to_be_downloaded))"
   ]
  }
 ],
 "metadata": {
  "celltoolbar": "Raw Cell Format",
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
